{"ast":null,"code":"import axios from \"axios\";\nimport _ from 'lodash';\nconst GPTService = {\n  async getAIResponse(messages) {\n    const prompt = getPrompt(messages);\n    const result = await getCompletion(prompt);\n    return result;\n  }\n\n};\nexport function trimLinesHelper(additional, lines, hardMax) {\n  let characterCount = additional;\n\n  const trimmedLines = _.takeRightWhile(lines, line => {\n    characterCount += line.length;\n    return characterCount <= hardMax;\n  });\n\n  return trimmedLines;\n}\n\nfunction trimLines(additional, lines) {\n  // As the chat continues, there's a tradeoff:\n  // More lines == higher cost + better result\n  // 2048 - 300 is upper bound for tokens.\n  // We will assume 1 token ~= 4 characters and keep a window of ~500 tokens.\n  const maxPromptLength = 500 * 4; // Davinci costs 0.06 per 1k tokens, so this is roughly 3 cents per completion at the upper end.\n\n  return trimLinesHelper(additional, lines, maxPromptLength);\n}\n\nfunction getPrompt(messages) {\n  const start = `The following is a conversation with a therapist. The assistant is helpful, creative, clever, and very friendly.\n\n`;\n  const additionalPrompt = \"AI:\";\n  const lines = messages.map(m => `${m.author}: ${m.message}\\n`);\n  const trimmed = trimLines(start.length + additionalPrompt.length, lines);\n  const combinedLines = trimmed.join(\"\");\n  return start + combinedLines + additionalPrompt;\n}\n\nconst RESPONSE_TOKEN_MAXIMUM = 300; // IMPORTANT: Please only use this for local testing. If you are deploying\n// your app onto the internet, you should route requests through your own\n// backend server to avoid exposing your OpenAI API key in your client\n// side code.\n\nasync function getCompletion(prompt) {\n  const data = {\n    prompt,\n    max_tokens: RESPONSE_TOKEN_MAXIMUM,\n    temperature: 0.9,\n    n: 1,\n    stop: ['AI:', `Human:`]\n  };\n  const result = await axios({\n    method: \"post\",\n    url: \"https://api.openai.com/v1/engines/davinci/completions\",\n    data,\n    headers: {\n      Authorization: \"Bearer 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\"\n    }\n  });\n  return result.data.choices[0].text;\n}\n\nexport default GPTService;","map":{"version":3,"sources":["/Users/alicecai/Desktop/gpt3-chat/src/services/gpt.ts"],"names":["axios","_","GPTService","getAIResponse","messages","prompt","getPrompt","result","getCompletion","trimLinesHelper","additional","lines","hardMax","characterCount","trimmedLines","takeRightWhile","line","length","trimLines","maxPromptLength","start","additionalPrompt","map","m","author","message","trimmed","combinedLines","join","RESPONSE_TOKEN_MAXIMUM","data","max_tokens","temperature","n","stop","method","url","headers","Authorization","choices","text"],"mappings":"AACA,OAAOA,KAAP,MAAkB,OAAlB;AACA,OAAOC,CAAP,MAAc,QAAd;AACA,MAAMC,UAAU,GAAG;AACf,QAAMC,aAAN,CAAoBC,QAApB,EAA0D;AACtD,UAAMC,MAAM,GAAGC,SAAS,CAACF,QAAD,CAAxB;AACA,UAAMG,MAAM,GAAG,MAAMC,aAAa,CAACH,MAAD,CAAlC;AACA,WAAOE,MAAP;AACH;;AALc,CAAnB;AAQA,OAAO,SAASE,eAAT,CAAyBC,UAAzB,EAA6CC,KAA7C,EAA8DC,OAA9D,EAAyF;AAC9F,MAAIC,cAAc,GAAGH,UAArB;;AACA,QAAMI,YAAY,GAAGb,CAAC,CAACc,cAAF,CAAiBJ,KAAjB,EAAyBK,IAAD,IAAU;AACrDH,IAAAA,cAAc,IAAIG,IAAI,CAACC,MAAvB;AACA,WAAOJ,cAAc,IAAID,OAAzB;AACD,GAHoB,CAArB;;AAIA,SAAOE,YAAP;AACD;;AAED,SAASI,SAAT,CAAmBR,UAAnB,EAAuCC,KAAvC,EAAkE;AAChE;AACA;AACA;AACA;AACA,QAAMQ,eAAe,GAAG,MAAM,CAA9B,CALgE,CAMhE;;AACA,SAAOV,eAAe,CAACC,UAAD,EAAaC,KAAb,EAAoBQ,eAApB,CAAtB;AACD;;AAED,SAASb,SAAT,CAAmBF,QAAnB,EAAgD;AAC9C,QAAMgB,KAAK,GAAI;AACjB;AACA,CAFE;AAGF,QAAMC,gBAAgB,GAAG,KAAzB;AAEE,QAAMV,KAAK,GAAGP,QAAQ,CAACkB,GAAT,CAAcC,CAAD,IAAQ,GAAEA,CAAC,CAACC,MAAO,KAAID,CAAC,CAACE,OAAQ,IAA9C,CAAd;AACA,QAAMC,OAAO,GAAGR,SAAS,CAACE,KAAK,CAACH,MAAN,GAAeI,gBAAgB,CAACJ,MAAjC,EAAyCN,KAAzC,CAAzB;AACA,QAAMgB,aAAa,GAAGD,OAAO,CAACE,IAAR,CAAa,EAAb,CAAtB;AAEA,SAAOR,KAAK,GAAGO,aAAR,GAAwBN,gBAA/B;AACD;;AAED,MAAMQ,sBAAsB,GAAG,GAA/B,C,CAEA;AACA;AACA;AACA;;AACA,eAAerB,aAAf,CAA6BH,MAA7B,EAA8D;AAC5D,QAAMyB,IAAI,GAAG;AACXzB,IAAAA,MADW;AAEX0B,IAAAA,UAAU,EAAEF,sBAFD;AAGXG,IAAAA,WAAW,EAAE,GAHF;AAIXC,IAAAA,CAAC,EAAE,CAJQ;AAKXC,IAAAA,IAAI,EAAE,CAAC,KAAD,EAAS,QAAT;AALK,GAAb;AAOA,QAAM3B,MAAM,GAAG,MAAMP,KAAK,CAAC;AACzBmC,IAAAA,MAAM,EAAE,MADiB;AAEzBC,IAAAA,GAAG,EAAE,uDAFoB;AAGzBN,IAAAA,IAHyB;AAIzBO,IAAAA,OAAO,EAAE;AACPC,MAAAA,aAAa,EAAE;AADR;AAJgB,GAAD,CAA1B;AAQA,SAAO/B,MAAM,CAACuB,IAAP,CAAYS,OAAZ,CAAoB,CAApB,EAAuBC,IAA9B;AACD;;AAED,eAAetC,UAAf","sourcesContent":["import { Message } from \"../components/MessageList\";\nimport axios from \"axios\";\nimport _ from 'lodash';\nconst GPTService = {\n    async getAIResponse(messages: Message[]): Promise<string> {\n        const prompt = getPrompt(messages);\n        const result = await getCompletion(prompt);\n        return result;\n    }\n};\n\nexport function trimLinesHelper(additional: number, lines: string[], hardMax: number): string[] {\n  let characterCount = additional;\n  const trimmedLines = _.takeRightWhile(lines, (line) => {\n    characterCount += line.length;\n    return characterCount <= hardMax;\n  });\n  return trimmedLines;\n}\n\nfunction trimLines(additional: number, lines: string[]): string[] {\n  // As the chat continues, there's a tradeoff:\n  // More lines == higher cost + better result\n  // 2048 - 300 is upper bound for tokens.\n  // We will assume 1 token ~= 4 characters and keep a window of ~500 tokens.\n  const maxPromptLength = 500 * 4;\n  // Davinci costs 0.06 per 1k tokens, so this is roughly 3 cents per completion at the upper end.\n  return trimLinesHelper(additional, lines, maxPromptLength);\n}\n\nfunction getPrompt(messages: Message[]): string {\n  const start = `The following is a conversation with a therapist. The assistant is helpful, creative, clever, and very friendly.\n\n`;\nconst additionalPrompt = \"AI:\";\n\n  const lines = messages.map((m) => `${m.author}: ${m.message}\\n`);\n  const trimmed = trimLines(start.length + additionalPrompt.length, lines);\n  const combinedLines = trimmed.join(\"\");\n  \n  return start + combinedLines + additionalPrompt;\n}\n\nconst RESPONSE_TOKEN_MAXIMUM = 300;\n\n// IMPORTANT: Please only use this for local testing. If you are deploying\n// your app onto the internet, you should route requests through your own\n// backend server to avoid exposing your OpenAI API key in your client\n// side code.\nasync function getCompletion(prompt: string): Promise<string> {\n  const data = {\n    prompt,\n    max_tokens: RESPONSE_TOKEN_MAXIMUM,\n    temperature: 0.9,\n    n: 1,\n    stop: ['AI:', `Human:`],\n  };\n  const result = await axios({\n    method: \"post\",\n    url: \"https://api.openai.com/v1/engines/davinci/completions\",\n    data,\n    headers: {\n      Authorization: \"Bearer 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\",\n    },\n  });\n  return result.data.choices[0].text;\n}\n\nexport default GPTService;\n"]},"metadata":{},"sourceType":"module"}